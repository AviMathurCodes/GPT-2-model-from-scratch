{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# MiniGPT2 by Avi Mathur\n",
        "\n",
        "# This notebook contains a full, training-ready PyTorch implementation of a compact GPT-2–style model (two Transformer layers by default).\n",
        "# Trained on the Tiny Shakespeare dataset (character-level) with: multi-head causal self-attention, positional encodings, AdamW optimizer,\n",
        "# LR warmup + cosine decay, mixed precision support, gradient accumulation, checkpointing, sampling (temperature / top-k / top-p), KV caching for fast generation.\n"
      ],
      "metadata": {
        "id": "z9X-t52DF5b2"
      },
      "id": "z9X-t52DF5b2",
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "583275b5-8569-41b7-a009-cedf9dfe589c",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "583275b5-8569-41b7-a009-cedf9dfe589c",
        "outputId": "3f16d6b6-4584-4a2b-92ef-2720f3675fbc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Device: cuda\n"
          ]
        }
      ],
      "source": [
        "## Cell 1 — Setup & Imports\n",
        "\n",
        "import os\n",
        "import math\n",
        "import time\n",
        "import json\n",
        "import random\n",
        "from pathlib import Path\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "# For mixed precision training\n",
        "from torch.cuda.amp import autocast, GradScaler\n",
        "\n",
        "# Reproducibility helper\n",
        "def set_seed(seed=42):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "set_seed(42)\n",
        "\n",
        "# Device\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "print('Device:', device)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Cell 2 — Config / Hyperparameters\n",
        "\n",
        "# Configuration dictionary\n",
        "class Config:\n",
        "    # data\n",
        "    dataset_url = 'https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt'\n",
        "    data_dir = Path('data')\n",
        "    dataset_file = data_dir / 'tiny_shakespeare.txt'\n",
        "    # model\n",
        "    vocab_size = None\n",
        "    block_size = 128  # context length\n",
        "    n_layers = 2      # number of transformer blocks\n",
        "    n_heads = 8\n",
        "    d_model = 256\n",
        "    d_ff = 1024\n",
        "    dropout = 0.1\n",
        "    # training\n",
        "    batch_size = 64\n",
        "    max_iters = 20000\n",
        "    eval_iters = 200\n",
        "    log_interval = 200\n",
        "    save_interval = 2000\n",
        "    learning_rate = 3e-4\n",
        "    betas = (0.9, 0.95)\n",
        "    weight_decay = 0.01\n",
        "    warmup_iters = 2000\n",
        "    device = device\n",
        "    # misc\n",
        "    grad_clip = 1.0\n",
        "    use_fp16 = True  # mixed precision\n",
        "    gradient_accumulation_steps = 1\n",
        "    out_dir = Path('out')\n",
        "\n",
        "config = Config()\n",
        "config.out_dir.mkdir(parents=True, exist_ok=True)\n",
        "print('Config prepared')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hhEGLUf8Di_N",
        "outputId": "82f7cbd1-a9ff-4956-88bd-3a74d085a4d1"
      },
      "id": "hhEGLUf8Di_N",
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Config prepared\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Cell 3 — Download dataset (Tiny Shakespeare) and build vocab (char-level)\n",
        "\n",
        "# Download the dataset if not present\n",
        "import urllib.request\n",
        "\n",
        "config.data_dir.mkdir(parents=True, exist_ok=True)\n",
        "if not config.dataset_file.exists():\n",
        "    print('Downloading dataset')\n",
        "    urllib.request.urlretrieve(config.dataset_url, config.dataset_file)\n",
        "    print('Downloaded to', config.dataset_file)\n",
        "else:\n",
        "    print('Dataset already exists at', config.dataset_file)\n",
        "\n",
        "# Read text\n",
        "with open(config.dataset_file, 'r', encoding='utf-8') as f:\n",
        "    text = f.read()\n",
        "\n",
        "print('Dataset length (chars):', len(text))\n",
        "\n",
        "# Build character vocabulary\n",
        "chars = sorted(list(set(text)))\n",
        "stoi = {ch:i for i,ch in enumerate(chars)}\n",
        "itos = {i:ch for ch,i in stoi.items()}\n",
        "config.vocab_size = len(chars)\n",
        "print('Vocab size:', config.vocab_size)\n",
        "\n",
        "# Save vocab\n",
        "vocab_file = config.out_dir / 'vocab.json'\n",
        "with open(vocab_file, 'w', encoding='utf-8') as f:\n",
        "    json.dump({'itos': itos, 'stoi': stoi}, f)\n",
        "print('Saved vocab to', vocab_file)\n",
        "\n",
        "# Encode full data\n",
        "data = np.array([stoi[c] for c in text], dtype=np.int64)\n",
        "\n",
        "# Train/validation split\n",
        "n = len(data)\n",
        "train_data = data[:int(n*0.9)]\n",
        "val_data = data[int(n*0.9):]\n",
        "print('Train tokens:', len(train_data), 'Val tokens:', len(val_data))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oYe-ws9YDjN0",
        "outputId": "5ac7bc68-43df-4f29-d23f-af217a54eb35"
      },
      "id": "oYe-ws9YDjN0",
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading dataset\n",
            "Downloaded to data/tiny_shakespeare.txt\n",
            "Dataset length (chars): 1115394\n",
            "Vocab size: 65\n",
            "Saved vocab to out/vocab.json\n",
            "Train tokens: 1003854 Val tokens: 111540\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Cell 4 - PyTorch Dataset & DataLoader for autoregressive batches\n",
        "\n",
        "class CharDataset(Dataset):\n",
        "    def __init__(self, data_array, block_size):\n",
        "        self.data = torch.from_numpy(data_array).long()\n",
        "        self.block_size = block_size\n",
        "    def __len__(self):\n",
        "        return max(1, len(self.data) - self.block_size)\n",
        "    def __getitem__(self, idx):\n",
        "        i = idx\n",
        "        x = self.data[i:i+self.block_size]\n",
        "        y = self.data[i+1:i+self.block_size+1]\n",
        "        return x, y\n",
        "\n",
        "train_dataset = CharDataset(train_data, config.block_size)\n",
        "val_dataset = CharDataset(val_data, config.block_size)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=config.batch_size, shuffle=True, drop_last=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=config.batch_size, shuffle=False, drop_last=True)\n",
        "\n",
        "print('Train loader batches:', len(train_loader))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fdMhctXGDjbn",
        "outputId": "8bb7b828-9aee-47a3-cdee-081c18ef480b"
      },
      "id": "fdMhctXGDjbn",
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loader batches: 15683\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Cell 5 - Model building blocks (Causal attention, MHA, Block, GPT)\n",
        "\n",
        "class CausalSelfAttention(nn.Module):\n",
        "    def __init__(self, d_model, n_heads, dropout):\n",
        "        super().__init__()\n",
        "        assert d_model % n_heads == 0, 'd_model must be divisible by n_heads'\n",
        "        self.n_heads = n_heads\n",
        "        self.head_dim = d_model // n_heads\n",
        "        self.scale = 1.0 / math.sqrt(self.head_dim)\n",
        "\n",
        "        # projections\n",
        "        self.qkv_proj = nn.Linear(d_model, 3 * d_model)\n",
        "        self.out_proj = nn.Linear(d_model, d_model)\n",
        "        self.attn_dropout = nn.Dropout(dropout)\n",
        "        self.resid_dropout = nn.Dropout(dropout)\n",
        "\n",
        "        # causal mask will be created on the fly for a given seq len\n",
        "        self.register_buffer('mask', None, persistent=False)\n",
        "\n",
        "    def _get_mask(self, T, device):\n",
        "        # create a lower-triangular mask 1's where allowed\n",
        "        if self.mask is None or self.mask.size(0) < T:\n",
        "            mask = torch.tril(torch.ones(T, T, dtype=torch.bool, device=device))\n",
        "            self.register_buffer('mask', mask, persistent=False)\n",
        "        return self.mask[:T, :T]\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, T, C = x.size()\n",
        "        qkv = self.qkv_proj(x)  # B, T, 3C\n",
        "        qkv = qkv.view(B, T, 3, self.n_heads, self.head_dim).permute(2, 0, 3, 1, 4)\n",
        "        q, k, v = qkv[0], qkv[1], qkv[2]  # each: B, n_heads, T, head_dim\n",
        "\n",
        "        # compute scaled dot-product attention\n",
        "        scores = torch.matmul(q, k.transpose(-2, -1)) * self.scale  # B, heads, T, T\n",
        "        mask = self._get_mask(T, x.device)\n",
        "        scores = scores.masked_fill(~mask.unsqueeze(0).unsqueeze(0), float('-inf'))\n",
        "        attn = F.softmax(scores, dim=-1)\n",
        "        attn = self.attn_dropout(attn)\n",
        "\n",
        "        y = torch.matmul(attn, v)  # B, heads, T, head_dim\n",
        "        y = y.transpose(1, 2).contiguous().view(B, T, C)\n",
        "        y = self.out_proj(y)\n",
        "        y = self.resid_dropout(y)\n",
        "        return y\n",
        "\n",
        "\n",
        "class FeedForward(nn.Module):\n",
        "    def __init__(self, d_model, d_ff, dropout):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(d_model, d_ff),\n",
        "            nn.GELU(),\n",
        "            nn.Linear(d_ff, d_model),\n",
        "            nn.Dropout(dropout)\n",
        "        )\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "\n",
        "class TransformerBlock(nn.Module):\n",
        "    def __init__(self, d_model, n_heads, d_ff, dropout):\n",
        "        super().__init__()\n",
        "        self.ln1 = nn.LayerNorm(d_model)\n",
        "        self.attn = CausalSelfAttention(d_model, n_heads, dropout)\n",
        "        self.ln2 = nn.LayerNorm(d_model)\n",
        "        self.ff = FeedForward(d_model, d_ff, dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # pre-norm transformer block\n",
        "        x = x + self.attn(self.ln1(x))\n",
        "        x = x + self.ff(self.ln2(x))\n",
        "        return x\n",
        "\n",
        "\n",
        "class MiniGPT(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.vocab_size = config.vocab_size\n",
        "        self.block_size = config.block_size\n",
        "        self.d_model = config.d_model\n",
        "\n",
        "        # token & position embeddings\n",
        "        self.tok_emb = nn.Embedding(self.vocab_size, self.d_model)\n",
        "        self.pos_emb = nn.Embedding(self.block_size, self.d_model)\n",
        "\n",
        "        self.drop = nn.Dropout(config.dropout)\n",
        "        # transformer blocks\n",
        "        self.blocks = nn.ModuleList([TransformerBlock(config.d_model, config.n_heads, config.d_ff, config.dropout)\n",
        "                                     for _ in range(config.n_layers)])\n",
        "        self.ln_f = nn.LayerNorm(self.d_model)\n",
        "\n",
        "        # language modeling head -> project to vocab\n",
        "        self.head = nn.Linear(self.d_model, self.vocab_size, bias=False)\n",
        "\n",
        "        # weight tying\n",
        "        self.head.weight = self.tok_emb.weight\n",
        "\n",
        "        self.apply(self._init_weights)\n",
        "\n",
        "    def _init_weights(self, module):\n",
        "        if isinstance(module, nn.Linear):\n",
        "            nn.init.xavier_uniform_(module.weight)\n",
        "            if module.bias is not None:\n",
        "                nn.init.zeros_(module.bias)\n",
        "        elif isinstance(module, nn.Embedding):\n",
        "            nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
        "        elif isinstance(module, nn.LayerNorm):\n",
        "            nn.init.ones_(module.weight)\n",
        "            nn.init.zeros_(module.bias)\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "        B, T = idx.size()\n",
        "        assert T <= self.block_size, 'Context length exceeds block size'\n",
        "\n",
        "        token_embeddings = self.tok_emb(idx)  # B, T, d\n",
        "        positions = torch.arange(T, device=idx.device)[None, :]\n",
        "        pos_embeddings = self.pos_emb(positions)\n",
        "        x = self.drop(token_embeddings + pos_embeddings)\n",
        "\n",
        "        for block in self.blocks:\n",
        "            x = block(x)\n",
        "\n",
        "        x = self.ln_f(x)\n",
        "        logits = self.head(x)  # B, T, vocab\n",
        "\n",
        "        loss = None\n",
        "        if targets is not None:\n",
        "            # flatten\n",
        "            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1))\n",
        "        return logits, loss\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def generate(self, idx, max_new_tokens, temperature=1.0, top_k=None, top_p=None):\n",
        "        # idx is (B, T) initial context\n",
        "        for _ in range(max_new_tokens):\n",
        "            idx_cond = idx[:, -self.block_size:]\n",
        "            logits, _ = self(idx_cond)\n",
        "            logits = logits[:, -1, :] / temperature\n",
        "\n",
        "            # top-k\n",
        "            if top_k is not None:\n",
        "                v, _ = torch.topk(logits, top_k)\n",
        "                min_v = v[:, -1].unsqueeze(1)\n",
        "                logits = torch.where(logits < min_v, torch.full_like(logits, float('-inf')), logits)\n",
        "\n",
        "            # top-p (nucleus)\n",
        "            if top_p is not None:\n",
        "                sorted_logits, sorted_indices = torch.sort(logits, descending=True, dim=-1)\n",
        "                cumulative_probs = torch.cumsum(F.softmax(sorted_logits, dim=-1), dim=-1)\n",
        "                # remove tokens with cumulative prob above threshold\n",
        "                sorted_indices_to_remove = cumulative_probs > top_p\n",
        "                # shift right to keep first token above threshold\n",
        "                sorted_indices_to_remove[..., 1:] = sorted_indices_to_remove[..., :-1].clone()\n",
        "                sorted_indices_to_remove[..., 0] = 0\n",
        "                indices_to_remove = sorted_indices_to_remove.scatter(1, sorted_indices, sorted_indices_to_remove)\n",
        "                logits = logits.masked_fill(indices_to_remove, float('-inf'))\n",
        "\n",
        "            probs = F.softmax(logits, dim=-1)\n",
        "            next_token = torch.multinomial(probs, num_samples=1)\n",
        "            idx = torch.cat((idx, next_token), dim=1)\n",
        "        return idx\n",
        "\n",
        "print('Model building blocks defined')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PiZfi96BDj0K",
        "outputId": "0cc5ab84-d69d-44db-ed63-fb78347ef47f"
      },
      "id": "PiZfi96BDj0K",
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model building blocks defined\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Cell 6 - Instantiate model, optimizer, scheduler, and AMP scaler\n",
        "\n",
        "model = MiniGPT(config).to(config.device)\n",
        "\n",
        "# count params\n",
        "def count_params(model):\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "print('Number of parameters:', count_params(model))\n",
        "\n",
        "# optimizer\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=config.learning_rate, betas=config.betas, weight_decay=config.weight_decay)\n",
        "\n",
        "# LR scheduler: linear warmup then cosine decay\n",
        "def get_lr(step):\n",
        "    if step < config.warmup_iters:\n",
        "        return config.learning_rate * step / max(1, config.warmup_iters)\n",
        "    # cosine decay after warmup\n",
        "    progress = (step - config.warmup_iters) / max(1, config.max_iters - config.warmup_iters)\n",
        "    return config.learning_rate * 0.5 * (1.0 + math.cos(math.pi * progress))\n",
        "\n",
        "scaler = GradScaler(enabled=config.use_fp16 and config.device == 'cuda')\n",
        "print('Setup complete')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HyhiE3kyDkAn",
        "outputId": "470698fb-e416-44fa-c012-5a3158a0701b"
      },
      "id": "HyhiE3kyDkAn",
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of parameters: 1629440\n",
            "Setup complete\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-1329056128.py:21: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  scaler = GradScaler(enabled=config.use_fp16 and config.device == 'cuda')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Cell 7 - Evaluation function (estimates validation loss)\n",
        "\n",
        "@torch.no_grad()\n",
        "def estimate_loss(model, val_loader, steps=200):\n",
        "    model.eval()\n",
        "    losses = []\n",
        "    it = 0\n",
        "    for x, y in val_loader:\n",
        "        x = x.to(config.device)\n",
        "        y = y.to(config.device)\n",
        "        logits, loss = model(x, y)\n",
        "        losses.append(loss.item())\n",
        "        it += 1\n",
        "        if it >= steps:\n",
        "            break\n",
        "    model.train()\n",
        "    return float(np.mean(losses))\n",
        "\n",
        "print('Eval function ready')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "36LYdPzvDkMG",
        "outputId": "97910619-44f8-4598-d723-34e48b5763ff"
      },
      "id": "36LYdPzvDkMG",
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Eval function ready\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Cell 8 - Training loop (with logging, fp16, grad accumulation, checkpointing)\n",
        "\n",
        "start_time = time.time()\n",
        "best_val_loss = 1e9\n",
        "iter_count = 0\n",
        "\n",
        "# Convert train_loader to infinite iterator\n",
        "def data_iterator(dataloader):\n",
        "    while True:\n",
        "        for batch in dataloader:\n",
        "            yield batch\n",
        "\n",
        "train_iter = data_iterator(train_loader)\n",
        "\n",
        "for iter_idx in range(config.max_iters):\n",
        "    iter_count += 1\n",
        "    model.train()\n",
        "\n",
        "    # get batch\n",
        "    xb, yb = next(train_iter)\n",
        "    xb = xb.to(config.device)\n",
        "    yb = yb.to(config.device)\n",
        "\n",
        "    # forward + backward with mixed precision\n",
        "    lr = get_lr(iter_idx)\n",
        "    for param_group in optimizer.param_groups:\n",
        "        param_group['lr'] = lr\n",
        "\n",
        "    with autocast(enabled=(config.use_fp16 and config.device=='cuda')):\n",
        "        logits, loss = model(xb, yb)\n",
        "        loss = loss / config.gradient_accumulation_steps\n",
        "\n",
        "    scaler.scale(loss).backward()\n",
        "\n",
        "    # gradient accumulation / step\n",
        "    if (iter_idx + 1) % config.gradient_accumulation_steps == 0:\n",
        "        # gradient clipping\n",
        "        scaler.unscale_(optimizer)\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), config.grad_clip)\n",
        "\n",
        "        scaler.step(optimizer)\n",
        "        scaler.update()\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "    # logging\n",
        "    if iter_idx % config.log_interval == 0:\n",
        "        elapsed = time.time() - start_time\n",
        "        print(f\"iter {iter_idx:8d} | time: {elapsed:7.1f}s | lr {lr:.2e} | loss {loss.item()*config.gradient_accumulation_steps:.4f}\")\n",
        "\n",
        "    # eval\n",
        "    if iter_idx % config.eval_iters == 0:\n",
        "        val_loss = estimate_loss(model, val_loader, steps= min(200, len(val_loader)))\n",
        "        print(f\"--- eval at iter {iter_idx} | val loss: {val_loss:.4f} | perplexity: {math.exp(val_loss):.2f}\")\n",
        "        if val_loss < best_val_loss:\n",
        "            best_val_loss = val_loss\n",
        "            # save best checkpoint\n",
        "            torch.save({'model_state_dict': model.state_dict(), 'optimizer_state_dict': optimizer.state_dict(), 'iter': iter_idx}, config.out_dir / 'best.pt')\n",
        "            print('Saved best checkpoint')\n",
        "\n",
        "    # periodic save\n",
        "    if iter_idx % config.save_interval == 0 and iter_idx > 0:\n",
        "        torch.save({'model_state_dict': model.state_dict(), 'optimizer_state_dict': optimizer.state_dict(), 'iter': iter_idx}, config.out_dir / f'iter_{iter_idx}.pt')\n",
        "\n",
        "print('Training finished')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "DgeMwXoLDkW7",
        "outputId": "88791302-6359-4424-da7d-20ff70478c9e"
      },
      "id": "DgeMwXoLDkW7",
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-939003924.py:29: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with autocast(enabled=(config.use_fp16 and config.device=='cuda')):\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "iter        0 | time:     1.2s | lr 0.00e+00 | loss 4.8482\n",
            "--- eval at iter 0 | val loss: 4.7914 | perplexity: 120.47\n",
            "Saved best checkpoint\n",
            "iter      200 | time:     7.8s | lr 3.00e-05 | loss 2.8907\n",
            "--- eval at iter 200 | val loss: 2.7770 | perplexity: 16.07\n",
            "Saved best checkpoint\n",
            "iter      400 | time:    14.2s | lr 6.00e-05 | loss 2.5615\n",
            "--- eval at iter 400 | val loss: 2.4802 | perplexity: 11.94\n",
            "Saved best checkpoint\n",
            "iter      600 | time:    21.0s | lr 9.00e-05 | loss 2.4850\n",
            "--- eval at iter 600 | val loss: 2.4301 | perplexity: 11.36\n",
            "Saved best checkpoint\n",
            "iter      800 | time:    27.4s | lr 1.20e-04 | loss 2.4198\n",
            "--- eval at iter 800 | val loss: 2.3924 | perplexity: 10.94\n",
            "Saved best checkpoint\n",
            "iter     1000 | time:    33.9s | lr 1.50e-04 | loss 2.3597\n",
            "--- eval at iter 1000 | val loss: 2.3514 | perplexity: 10.50\n",
            "Saved best checkpoint\n",
            "iter     1200 | time:    40.4s | lr 1.80e-04 | loss 2.2928\n",
            "--- eval at iter 1200 | val loss: 2.2796 | perplexity: 9.77\n",
            "Saved best checkpoint\n",
            "iter     1400 | time:    46.9s | lr 2.10e-04 | loss 2.2330\n",
            "--- eval at iter 1400 | val loss: 2.1817 | perplexity: 8.86\n",
            "Saved best checkpoint\n",
            "iter     1600 | time:    53.4s | lr 2.40e-04 | loss 2.1459\n",
            "--- eval at iter 1600 | val loss: 2.0879 | perplexity: 8.07\n",
            "Saved best checkpoint\n",
            "iter     1800 | time:    59.9s | lr 2.70e-04 | loss 2.0449\n",
            "--- eval at iter 1800 | val loss: 1.9728 | perplexity: 7.19\n",
            "Saved best checkpoint\n",
            "iter     2000 | time:    66.5s | lr 3.00e-04 | loss 1.9393\n",
            "--- eval at iter 2000 | val loss: 1.9122 | perplexity: 6.77\n",
            "Saved best checkpoint\n",
            "iter     2200 | time:    73.1s | lr 3.00e-04 | loss 1.8960\n",
            "--- eval at iter 2200 | val loss: 1.8366 | perplexity: 6.28\n",
            "Saved best checkpoint\n",
            "iter     2400 | time:    79.7s | lr 3.00e-04 | loss 1.8313\n",
            "--- eval at iter 2400 | val loss: 1.7867 | perplexity: 5.97\n",
            "Saved best checkpoint\n",
            "iter     2600 | time:    86.3s | lr 3.00e-04 | loss 1.7930\n",
            "--- eval at iter 2600 | val loss: 1.7341 | perplexity: 5.66\n",
            "Saved best checkpoint\n",
            "iter     2800 | time:    93.0s | lr 3.00e-04 | loss 1.7725\n",
            "--- eval at iter 2800 | val loss: 1.6887 | perplexity: 5.41\n",
            "Saved best checkpoint\n",
            "iter     3000 | time:    99.7s | lr 3.00e-04 | loss 1.7661\n",
            "--- eval at iter 3000 | val loss: 1.6585 | perplexity: 5.25\n",
            "Saved best checkpoint\n",
            "iter     3200 | time:   106.5s | lr 3.00e-04 | loss 1.6555\n",
            "--- eval at iter 3200 | val loss: 1.6255 | perplexity: 5.08\n",
            "Saved best checkpoint\n",
            "iter     3400 | time:   113.2s | lr 3.00e-04 | loss 1.6294\n",
            "--- eval at iter 3400 | val loss: 1.6043 | perplexity: 4.97\n",
            "Saved best checkpoint\n",
            "iter     3600 | time:   120.0s | lr 3.00e-04 | loss 1.5958\n",
            "--- eval at iter 3600 | val loss: 1.5942 | perplexity: 4.92\n",
            "Saved best checkpoint\n",
            "iter     3800 | time:   126.8s | lr 3.00e-04 | loss 1.6091\n",
            "--- eval at iter 3800 | val loss: 1.5577 | perplexity: 4.75\n",
            "Saved best checkpoint\n",
            "iter     4000 | time:   133.6s | lr 3.00e-04 | loss 1.5399\n",
            "--- eval at iter 4000 | val loss: 1.5372 | perplexity: 4.65\n",
            "Saved best checkpoint\n",
            "iter     4200 | time:   140.5s | lr 3.00e-04 | loss 1.5443\n",
            "--- eval at iter 4200 | val loss: 1.5121 | perplexity: 4.54\n",
            "Saved best checkpoint\n",
            "iter     4400 | time:   147.3s | lr 3.00e-04 | loss 1.5209\n",
            "--- eval at iter 4400 | val loss: 1.5100 | perplexity: 4.53\n",
            "Saved best checkpoint\n",
            "iter     4600 | time:   154.2s | lr 3.00e-04 | loss 1.5350\n",
            "--- eval at iter 4600 | val loss: 1.4909 | perplexity: 4.44\n",
            "Saved best checkpoint\n",
            "iter     4800 | time:   161.0s | lr 3.00e-04 | loss 1.5397\n",
            "--- eval at iter 4800 | val loss: 1.4790 | perplexity: 4.39\n",
            "Saved best checkpoint\n",
            "iter     5000 | time:   167.8s | lr 3.00e-04 | loss 1.4735\n",
            "--- eval at iter 5000 | val loss: 1.4547 | perplexity: 4.28\n",
            "Saved best checkpoint\n",
            "iter     5200 | time:   174.6s | lr 3.00e-04 | loss 1.4616\n",
            "--- eval at iter 5200 | val loss: 1.4480 | perplexity: 4.25\n",
            "Saved best checkpoint\n",
            "iter     5400 | time:   181.4s | lr 3.00e-04 | loss 1.4447\n",
            "--- eval at iter 5400 | val loss: 1.4526 | perplexity: 4.27\n",
            "iter     5600 | time:   188.1s | lr 3.00e-04 | loss 1.4472\n",
            "--- eval at iter 5600 | val loss: 1.4415 | perplexity: 4.23\n",
            "Saved best checkpoint\n",
            "iter     5800 | time:   194.9s | lr 3.00e-04 | loss 1.3890\n",
            "--- eval at iter 5800 | val loss: 1.4369 | perplexity: 4.21\n",
            "Saved best checkpoint\n",
            "iter     6000 | time:   201.8s | lr 3.00e-04 | loss 1.4754\n",
            "--- eval at iter 6000 | val loss: 1.4343 | perplexity: 4.20\n",
            "Saved best checkpoint\n",
            "iter     6200 | time:   208.6s | lr 3.00e-04 | loss 1.3789\n",
            "--- eval at iter 6200 | val loss: 1.4248 | perplexity: 4.16\n",
            "Saved best checkpoint\n",
            "iter     6400 | time:   215.4s | lr 3.00e-04 | loss 1.3997\n",
            "--- eval at iter 6400 | val loss: 1.4132 | perplexity: 4.11\n",
            "Saved best checkpoint\n",
            "iter     6600 | time:   222.2s | lr 3.00e-04 | loss 1.4080\n",
            "--- eval at iter 6600 | val loss: 1.4094 | perplexity: 4.09\n",
            "Saved best checkpoint\n",
            "iter     6800 | time:   229.0s | lr 3.00e-04 | loss 1.3614\n",
            "--- eval at iter 6800 | val loss: 1.4082 | perplexity: 4.09\n",
            "Saved best checkpoint\n",
            "iter     7000 | time:   235.9s | lr 3.00e-04 | loss 1.4180\n",
            "--- eval at iter 7000 | val loss: 1.4067 | perplexity: 4.08\n",
            "Saved best checkpoint\n",
            "iter     7200 | time:   242.6s | lr 2.99e-04 | loss 1.4070\n",
            "--- eval at iter 7200 | val loss: 1.4081 | perplexity: 4.09\n",
            "iter     7400 | time:   249.4s | lr 2.99e-04 | loss 1.3690\n",
            "--- eval at iter 7400 | val loss: 1.3946 | perplexity: 4.03\n",
            "Saved best checkpoint\n",
            "iter     7600 | time:   256.2s | lr 2.99e-04 | loss 1.4033\n",
            "--- eval at iter 7600 | val loss: 1.3949 | perplexity: 4.03\n",
            "iter     7800 | time:   262.9s | lr 2.99e-04 | loss 1.3503\n",
            "--- eval at iter 7800 | val loss: 1.3948 | perplexity: 4.03\n",
            "iter     8000 | time:   269.7s | lr 2.99e-04 | loss 1.3495\n",
            "--- eval at iter 8000 | val loss: 1.3921 | perplexity: 4.02\n",
            "Saved best checkpoint\n",
            "iter     8200 | time:   276.5s | lr 2.99e-04 | loss 1.3483\n",
            "--- eval at iter 8200 | val loss: 1.3951 | perplexity: 4.04\n",
            "iter     8400 | time:   283.3s | lr 2.99e-04 | loss 1.3529\n",
            "--- eval at iter 8400 | val loss: 1.3901 | perplexity: 4.02\n",
            "Saved best checkpoint\n",
            "iter     8600 | time:   290.1s | lr 2.99e-04 | loss 1.3305\n",
            "--- eval at iter 8600 | val loss: 1.3859 | perplexity: 4.00\n",
            "Saved best checkpoint\n",
            "iter     8800 | time:   297.0s | lr 2.99e-04 | loss 1.3362\n",
            "--- eval at iter 8800 | val loss: 1.3823 | perplexity: 3.98\n",
            "Saved best checkpoint\n",
            "iter     9000 | time:   303.7s | lr 2.99e-04 | loss 1.3420\n",
            "--- eval at iter 9000 | val loss: 1.3804 | perplexity: 3.98\n",
            "Saved best checkpoint\n",
            "iter     9200 | time:   310.6s | lr 2.99e-04 | loss 1.3375\n",
            "--- eval at iter 9200 | val loss: 1.3823 | perplexity: 3.98\n",
            "iter     9400 | time:   317.3s | lr 2.99e-04 | loss 1.3462\n",
            "--- eval at iter 9400 | val loss: 1.3776 | perplexity: 3.97\n",
            "Saved best checkpoint\n",
            "iter     9600 | time:   324.1s | lr 2.99e-04 | loss 1.3620\n",
            "--- eval at iter 9600 | val loss: 1.3759 | perplexity: 3.96\n",
            "Saved best checkpoint\n",
            "iter     9800 | time:   331.0s | lr 2.99e-04 | loss 1.3276\n",
            "--- eval at iter 9800 | val loss: 1.3723 | perplexity: 3.94\n",
            "Saved best checkpoint\n",
            "iter    10000 | time:   337.8s | lr 2.99e-04 | loss 1.3022\n",
            "--- eval at iter 10000 | val loss: 1.3626 | perplexity: 3.91\n",
            "Saved best checkpoint\n",
            "iter    10200 | time:   344.6s | lr 2.99e-04 | loss 1.3038\n",
            "--- eval at iter 10200 | val loss: 1.3750 | perplexity: 3.95\n",
            "iter    10400 | time:   351.4s | lr 2.99e-04 | loss 1.3132\n",
            "--- eval at iter 10400 | val loss: 1.3664 | perplexity: 3.92\n",
            "iter    10600 | time:   358.1s | lr 2.99e-04 | loss 1.3096\n",
            "--- eval at iter 10600 | val loss: 1.3679 | perplexity: 3.93\n",
            "iter    10800 | time:   364.9s | lr 2.99e-04 | loss 1.3115\n",
            "--- eval at iter 10800 | val loss: 1.3704 | perplexity: 3.94\n",
            "iter    11000 | time:   371.6s | lr 2.98e-04 | loss 1.3047\n",
            "--- eval at iter 11000 | val loss: 1.3679 | perplexity: 3.93\n",
            "iter    11200 | time:   378.4s | lr 2.98e-04 | loss 1.3189\n",
            "--- eval at iter 11200 | val loss: 1.3639 | perplexity: 3.91\n",
            "iter    11400 | time:   385.2s | lr 2.98e-04 | loss 1.2775\n",
            "--- eval at iter 11400 | val loss: 1.3559 | perplexity: 3.88\n",
            "Saved best checkpoint\n",
            "iter    11600 | time:   392.0s | lr 2.98e-04 | loss 1.2769\n",
            "--- eval at iter 11600 | val loss: 1.3685 | perplexity: 3.93\n",
            "iter    11800 | time:   398.7s | lr 2.98e-04 | loss 1.2712\n",
            "--- eval at iter 11800 | val loss: 1.3659 | perplexity: 3.92\n",
            "iter    12000 | time:   405.5s | lr 2.98e-04 | loss 1.2724\n",
            "--- eval at iter 12000 | val loss: 1.3624 | perplexity: 3.91\n",
            "iter    12200 | time:   412.3s | lr 2.98e-04 | loss 1.2849\n",
            "--- eval at iter 12200 | val loss: 1.3573 | perplexity: 3.89\n",
            "iter    12400 | time:   419.1s | lr 2.98e-04 | loss 1.2754\n",
            "--- eval at iter 12400 | val loss: 1.3563 | perplexity: 3.88\n",
            "iter    12600 | time:   425.8s | lr 2.98e-04 | loss 1.2565\n",
            "--- eval at iter 12600 | val loss: 1.3568 | perplexity: 3.88\n",
            "iter    12800 | time:   432.6s | lr 2.98e-04 | loss 1.3048\n",
            "--- eval at iter 12800 | val loss: 1.3577 | perplexity: 3.89\n",
            "iter    13000 | time:   439.4s | lr 2.98e-04 | loss 1.2927\n",
            "--- eval at iter 13000 | val loss: 1.3547 | perplexity: 3.88\n",
            "Saved best checkpoint\n",
            "iter    13200 | time:   446.2s | lr 2.98e-04 | loss 1.2970\n",
            "--- eval at iter 13200 | val loss: 1.3527 | perplexity: 3.87\n",
            "Saved best checkpoint\n",
            "iter    13400 | time:   453.1s | lr 2.98e-04 | loss 1.2708\n",
            "--- eval at iter 13400 | val loss: 1.3484 | perplexity: 3.85\n",
            "Saved best checkpoint\n",
            "iter    13600 | time:   459.8s | lr 2.97e-04 | loss 1.3030\n",
            "--- eval at iter 13600 | val loss: 1.3538 | perplexity: 3.87\n",
            "iter    13800 | time:   466.6s | lr 2.97e-04 | loss 1.2721\n",
            "--- eval at iter 13800 | val loss: 1.3574 | perplexity: 3.89\n",
            "iter    14000 | time:   473.4s | lr 2.97e-04 | loss 1.2591\n",
            "--- eval at iter 14000 | val loss: 1.3548 | perplexity: 3.88\n",
            "iter    14200 | time:   480.2s | lr 2.97e-04 | loss 1.2543\n",
            "--- eval at iter 14200 | val loss: 1.3542 | perplexity: 3.87\n",
            "iter    14400 | time:   487.0s | lr 2.97e-04 | loss 1.2417\n",
            "--- eval at iter 14400 | val loss: 1.3606 | perplexity: 3.90\n",
            "iter    14600 | time:   493.7s | lr 2.97e-04 | loss 1.2672\n",
            "--- eval at iter 14600 | val loss: 1.3580 | perplexity: 3.89\n",
            "iter    14800 | time:   500.5s | lr 2.97e-04 | loss 1.2572\n",
            "--- eval at iter 14800 | val loss: 1.3537 | perplexity: 3.87\n",
            "iter    15000 | time:   507.3s | lr 2.97e-04 | loss 1.2582\n",
            "--- eval at iter 15000 | val loss: 1.3557 | perplexity: 3.88\n",
            "iter    15200 | time:   514.1s | lr 2.97e-04 | loss 1.2084\n",
            "--- eval at iter 15200 | val loss: 1.3544 | perplexity: 3.87\n",
            "iter    15400 | time:   520.8s | lr 2.97e-04 | loss 1.2291\n",
            "--- eval at iter 15400 | val loss: 1.3586 | perplexity: 3.89\n",
            "iter    15600 | time:   527.6s | lr 2.97e-04 | loss 1.2017\n",
            "--- eval at iter 15600 | val loss: 1.3555 | perplexity: 3.88\n",
            "iter    15800 | time:   534.5s | lr 2.96e-04 | loss 1.2511\n",
            "--- eval at iter 15800 | val loss: 1.3560 | perplexity: 3.88\n",
            "iter    16000 | time:   541.3s | lr 2.96e-04 | loss 1.2310\n",
            "--- eval at iter 16000 | val loss: 1.3635 | perplexity: 3.91\n",
            "iter    16200 | time:   548.1s | lr 2.96e-04 | loss 1.2572\n",
            "--- eval at iter 16200 | val loss: 1.3493 | perplexity: 3.85\n",
            "iter    16400 | time:   554.9s | lr 2.96e-04 | loss 1.2639\n",
            "--- eval at iter 16400 | val loss: 1.3515 | perplexity: 3.86\n",
            "iter    16600 | time:   561.7s | lr 2.96e-04 | loss 1.2713\n",
            "--- eval at iter 16600 | val loss: 1.3531 | perplexity: 3.87\n",
            "iter    16800 | time:   568.4s | lr 2.96e-04 | loss 1.2362\n",
            "--- eval at iter 16800 | val loss: 1.3611 | perplexity: 3.90\n",
            "iter    17000 | time:   575.2s | lr 2.96e-04 | loss 1.1953\n",
            "--- eval at iter 17000 | val loss: 1.3588 | perplexity: 3.89\n",
            "iter    17200 | time:   581.9s | lr 2.96e-04 | loss 1.2199\n",
            "--- eval at iter 17200 | val loss: 1.3597 | perplexity: 3.90\n",
            "iter    17400 | time:   588.7s | lr 2.96e-04 | loss 1.2388\n",
            "--- eval at iter 17400 | val loss: 1.3519 | perplexity: 3.86\n",
            "iter    17600 | time:   595.5s | lr 2.95e-04 | loss 1.2645\n",
            "--- eval at iter 17600 | val loss: 1.3478 | perplexity: 3.85\n",
            "Saved best checkpoint\n",
            "iter    17800 | time:   602.3s | lr 2.95e-04 | loss 1.2534\n",
            "--- eval at iter 17800 | val loss: 1.3525 | perplexity: 3.87\n",
            "iter    18000 | time:   609.1s | lr 2.95e-04 | loss 1.2500\n",
            "--- eval at iter 18000 | val loss: 1.3500 | perplexity: 3.86\n",
            "iter    18200 | time:   615.9s | lr 2.95e-04 | loss 1.2407\n",
            "--- eval at iter 18200 | val loss: 1.3537 | perplexity: 3.87\n",
            "iter    18400 | time:   622.6s | lr 2.95e-04 | loss 1.2324\n",
            "--- eval at iter 18400 | val loss: 1.3489 | perplexity: 3.85\n",
            "iter    18600 | time:   629.4s | lr 2.95e-04 | loss 1.2058\n",
            "--- eval at iter 18600 | val loss: 1.3560 | perplexity: 3.88\n",
            "iter    18800 | time:   636.2s | lr 2.95e-04 | loss 1.2099\n",
            "--- eval at iter 18800 | val loss: 1.3523 | perplexity: 3.87\n",
            "iter    19000 | time:   643.0s | lr 2.95e-04 | loss 1.2076\n",
            "--- eval at iter 19000 | val loss: 1.3607 | perplexity: 3.90\n",
            "iter    19200 | time:   649.7s | lr 2.94e-04 | loss 1.2208\n",
            "--- eval at iter 19200 | val loss: 1.3451 | perplexity: 3.84\n",
            "Saved best checkpoint\n",
            "iter    19400 | time:   656.6s | lr 2.94e-04 | loss 1.2856\n",
            "--- eval at iter 19400 | val loss: 1.3468 | perplexity: 3.85\n",
            "iter    19600 | time:   663.3s | lr 2.94e-04 | loss 1.2324\n",
            "--- eval at iter 19600 | val loss: 1.3508 | perplexity: 3.86\n",
            "iter    19800 | time:   670.1s | lr 2.94e-04 | loss 1.2038\n",
            "--- eval at iter 19800 | val loss: 1.3559 | perplexity: 3.88\n",
            "iter    20000 | time:   676.9s | lr 2.94e-04 | loss 1.2298\n",
            "--- eval at iter 20000 | val loss: 1.3445 | perplexity: 3.84\n",
            "Saved best checkpoint\n"
          ]
        },
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Cell 9 - Sampling demo (load checkpoint and generate samples)\n",
        "\n",
        "# Load checkpoint (best or last)\n",
        "ckpt_path = config.out_dir / 'best.pt'\n",
        "if ckpt_path.exists():\n",
        "    ckpt = torch.load(ckpt_path, map_location=config.device)\n",
        "    model.load_state_dict(ckpt['model_state_dict'])\n",
        "    print('Loaded checkpoint from', ckpt_path)\n",
        "else:\n",
        "    print('No checkpoint found at', ckpt_path)\n",
        "\n",
        "# helper encode/decode\n",
        "def encode(s):\n",
        "    return torch.tensor([stoi[c] for c in s], dtype=torch.long, device=config.device).unsqueeze(0)\n",
        "\n",
        "def decode(ids):\n",
        "    return ''.join([itos[int(i)] for i in ids])\n",
        "\n",
        "# generate\n",
        "model.eval()\n",
        "context = 'ROMEO: '\n",
        "context_ids = encode(context)\n",
        "out_ids = model.generate(context_ids, max_new_tokens=500, temperature=0.9, top_k=40, top_p=0.9)\n",
        "print('SAMPLE:')\n",
        "print(decode(out_ids[0].cpu().numpy()))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zDt7HAh9Dkhb",
        "outputId": "34006db9-cdca-4cb8-8a73-9b012d16a99f"
      },
      "id": "zDt7HAh9Dkhb",
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded checkpoint from out/best.pt\n",
            "SAMPLE:\n",
            "ROMEO: it is, my lord?\n",
            "\n",
            "POMPEY:\n",
            "I have been no foul thus: the princess is the merit.\n",
            "\n",
            "ESCALUS:\n",
            "I am too hardly of your own daughter to him: he'll keep you.\n",
            "\n",
            "LUCIO:\n",
            "Good master, sir.\n",
            "\n",
            "ESCALUS:\n",
            "The gates of your father's head, and made themselves;\n",
            "And yet you might at all shame for me; I shall have\n",
            "In your grace, and your counsel to a court,\n",
            "And stand upon your life is lender from your master,\n",
            "And then your body and could no other way;\n",
            "For I shall be the king house of York.\n",
            "\n",
            "KING RICHARD III:\n",
            "Madam, he i\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Cell 10 - Tips, next steps, and hyperparameter suggestions\n",
        "\n",
        "'''\n",
        "Tips for scaling & improving:\n",
        "\n",
        "- Use `gradient_accumulation_steps` to simulate larger batch sizes if GPU memory is limited.\n",
        "- Enable `use_fp16=True` for faster training and memory savings on modern GPUs.\n",
        "- Increasing `n_layers`, `d_model`, and `d_ff` for better capacity.\n",
        "- Add KV-caching in attention for much faster autoregressive generation.\n",
        "- Replace learned positional embedding with rotary or relative positional encodings for improved extrapolation.\n",
        "- Use FlashAttention kernels or optimized libraries for large-scale training.\n",
        "'''"
      ],
      "metadata": {
        "id": "ffpqfQvwDks7"
      },
      "id": "ffpqfQvwDks7",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.9"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
